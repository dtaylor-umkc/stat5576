{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b01c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Monte Carlo Simulation\n",
    "### A Measure‑Theoretic Probability Perspective\n",
    "*Your Name* – Graduate Probability Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee87b17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. History of Monte Carlo Simulation\n",
    "2. Mathematical Foundations\n",
    "3. Monte Carlo Variants & Variance Reduction\n",
    "4. GPU Acceleration\n",
    "5. Examples & Live Code\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66877de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief History (1/2)\n",
    "- 18th century: **Buffon's Needle** experiment estimates π.\n",
    "- Name derives from the Monte Carlo casino, alluding to randomness.\n",
    "- 1930s: Early random sampling in statistics (Fisher, Neyman).\n",
    "- 1940s: Stanislaw **Ulam** & **John von Neumann** formalize MC during the Manhattan Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a59e57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brief History (2/2)\n",
    "- 1949: **Metropolis & Ulam** paper establishes formal MC method.\n",
    "- 1950s–70s: Integration in physics (neutron transport), finance (option pricing).\n",
    "- 1990s–present: HPC & GPUs accelerate MC; quasi‑MC & variance reduction refine accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0428",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Core Idea\n",
    "Use random samples $X_1,\\dots,X_n$ to approximate an expectation:\n",
    "$$\\mathbb{E}[f(X)] \\approx \\frac1n\\sum_{i=1}^n f(X_i)$$\n",
    "Any quantity expressible as an integral can be estimated this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1420b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measure‑Theoretic Foundation\n",
    "Let $(\\Omega,\\mathcal F,\\mathbb P)$ be a probability space and $f\\!:\\!\\Omega\\to\\mathbb R$ with $f\\in L^1$.\n",
    "\n",
    "**Strong Law of Large Numbers (Kolmogorov, 1933)**:\n",
    "$$\\frac1n\\sum_{i=1}^n f(X_i) \\xrightarrow{\\text{a.s.}} \\mathbb{E}[f(X)].$$\n",
    "\n",
    "This justifies Monte Carlo estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c793b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error & Convergence\n",
    "- **Central Limit Theorem** (CLT) gives asymptotic normality:\n",
    "$$\\sqrt{n}\\bigl(\\bar f_n-\\mu\\bigr) \\xrightarrow{d} \\mathcal N(0,\\sigma^2), \\; \\sigma^2=\\operatorname{Var}(f(X)).$$\n",
    "- Root‑Mean‑Square error decays like $\\mathcal O(n^{-1/2})$ → slow but dimension‑free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29073969",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic Monte Carlo Algorithm\n",
    "1. Draw i.i.d. samples $X_1,\\dots,X_n$ from $p(x)$.\n",
    "2. Compute $f(X_i)$.\n",
    "3. Return estimator $\\hat\\mu_n = \\frac1n\\sum f(X_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154abb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo Variants\n",
    "- **Crude MC** (plain sampling)\n",
    "- **Quasi‑MC** (low‑discrepancy sequences)\n",
    "- **Markov Chain MC** (MCMC)\n",
    "- **Sequential MC** (particle filters)\n",
    "- **Multilevel MC** (hierarchical discretizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496814b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance Reduction Techniques\n",
    "- **Antithetic variates**\n",
    "- **Control variates**\n",
    "- **Importance sampling**\n",
    "- **Stratified / Latin hypercube sampling**\n",
    "- **Common random numbers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654919d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU Acceleration\n",
    "- Thousands of CUDA cores → massive parallelism\n",
    "- Throughput model fits independent simulations\n",
    "- Speed‑ups of 10×–100× for high‑dimensional problems\n",
    "\n",
    "![GPU](https://upload.wikimedia.org/wikipedia/commons/2/2a/Nvidia-logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f525d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1: Estimating $\\pi$ by Dart‑Throwing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c418abb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mc_pi(n_samples=100_000):\n",
    "    x = np.random.rand(n_samples)\n",
    "    y = np.random.rand(n_samples)\n",
    "    return 4 * np.mean(x**2 + y**2 < 1)\n",
    "\n",
    "for n in [10**k for k in range(1, 7)]:\n",
    "    print(f\"{n:>7}: pi ≈ {mc_pi(n):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42f228",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convergence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e2513",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_values = np.logspace(1, 6, 60, dtype=int)\n",
    "errors = [abs(mc_pi(n) - np.pi) for n in n_values]\n",
    "plt.loglog(n_values, errors)\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"|π̂ − π|\")\n",
    "plt.title(\"Convergence of Crude Monte Carlo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8639a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Antithetic Variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f7753",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mc_pi_antithetic(n_samples=100_000):\n",
    "    half = n_samples // 2\n",
    "    x = np.random.rand(half)\n",
    "    y = np.random.rand(half)\n",
    "    x2 = 1 - x\n",
    "    y2 = 1 - y\n",
    "    inside = np.concatenate(((x**2 + y**2) < 1, (x2**2 + y2**2) < 1))\n",
    "    return 4 * inside.mean()\n",
    "\n",
    "for n in [10**k for k in range(1, 7)]:\n",
    "    crude = mc_pi(n)\n",
    "    anti = mc_pi_antithetic(n)\n",
    "    print(f\"{n:>7}: crude {crude:.6f}    antithetic {anti:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79fad0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_values = np.logspace(1, 6, 60, dtype=int)\n",
    "errors_crude = [abs(mc_pi(n) - np.pi) for n in n_values]\n",
    "errors_anti = [abs(mc_pi_antithetic(n) - np.pi) for n in n_values]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(n_values, errors_crude, label=\"Crude\")\n",
    "plt.loglog(n_values, errors_anti, label=\"Antithetic\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Absolute Error\")\n",
    "plt.title(\"Variance Reduction with Antithetic Variates\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc864d31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Framework: Homework Example 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9c8b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*(Insert problem statement here)*\n",
    "\n",
    "### Template Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ce67c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# MC solution template for Homework 4b\n",
    "import numpy as np\n",
    "# TODO: define problem-specific function f(x)\n",
    "# TODO: sample from distribution p(x)\n",
    "# TODO: compute estimator and confidence interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad8e36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Framework: Homework Example 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e175b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MC solution template for Homework 4c\n",
    "# TODO: implement problem-specific sampling and estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ff4ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Framework: Homework Example 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdb6c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MC solution template for Homework 4d\n",
    "# TODO: implement problem-specific sampling and estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3294fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- MC turns integration into averaging random draws.\n",
    "- Converges at $n^{-1/2}$; variance reduction & GPUs mitigate computational cost.\n",
    "- Measure‑theoretic probability provides rigorous guarantees.\n",
    "- Applications span physics, finance, risk analysis, and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd78a11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "- Metropolis, N. & Ulam, S. (1949). *The Monte Carlo Method*. JASA.\n",
    "- Kalos, M. H. & Whitlock, P. A. (2008). *Monte Carlo Methods*. Wiley.\n",
    "- Robert, C. P. & Casella, G. (2004). *Monte Carlo Statistical Methods*. Springer.\n",
    "- Glasserman, P. (2003). *Monte Carlo Methods in Financial Engineering*. Springer.\n",
    "- Owen, A. B. (2013). *Monte Carlo Theory, Methods and Examples*.\n",
    "- NVIDIA Corporation (2021). *CUDA Programming Guide*."
   ]
  }
 ],
 "metadata": {
  "rise": {
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
