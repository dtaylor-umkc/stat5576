{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Brief Overview of Monte Carlo Simulation\n",
    "\n",
    "### Presenter: Demetrius Taylor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. History of Monte Carlo Simulation\n",
    "2. Mathematical Foundations\n",
    "3. Monte Carlo Variants\n",
    "4. Variance Reduction Methods\n",
    "5. Working Examples\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# History of Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brief History (1 of 2)\n",
    "\n",
    "Monte Carlo Simulation was developed in the 1940's by physicist Stanislaw Ulam and mathematician John von Neumann\n",
    "\n",
    "Used to model model neutron diffusion for nuclear weapons development as part of the Manhattan Project during World War II\n",
    "\n",
    "Stanislaw Ulam | John von Neumann\n",
    "- | -\n",
    "<img src=\"ulam.jpeg\" width=\"700\" /> | <img src=\"vonneumann.jpg\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brief History (2 of 2)\n",
    "\n",
    "The name was inspired by the Monte Carlo Casino in Monaco due to the method's reliance on random numbers\n",
    "\n",
    "\"Monaco on the French Riviera\" | \"Casino de Monte-Carlo (aka The Monte Carlo Casino)\"\n",
    "- | -\n",
    "![alt text](monaco.jpg) | ![alt text](montecarlo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mathematical Foundations (1 of 4)\n",
    "\n",
    "When we run a Monte Carlo simulation we usually want to approximate an expected value\n",
    "\n",
    "$$\\mu = \\mathbb{E}[f(X)]$$\n",
    "\n",
    "We do this by drawing $n$ independent random samples $X_1, \\ldots, X_n$ from the distribution of $X$ and computing the sample mean\n",
    "\n",
    "$$\\hat{\\mu}_{n} = \\frac{1}{n} \\sum_{i=1}^n f(X_i) \\approx \\mathbb{E}[f(X)]$$\n",
    "\n",
    "Since we know from class that the expectation is basically an integral, any quantity expressible as an integral can be estimated this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mathematical Foundations (2 of 4)\n",
    "\n",
    "The justification of Monte Carlo estimators is the Strong Law of Large Numbers (Kolmogorov, 1933)\n",
    "\n",
    "Let $(\\Omega,\\mathcal F,\\mathbb P)$ be a probability space and $f\\!:\\!\\Omega\\to\\mathbb R$ with $f\\in L^1$\n",
    "\n",
    "$$\\frac1n\\sum_{i=1}^n f(X_i) \\xrightarrow{\\text{a.s.}} \\mathbb{E}[f(X)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mathematical Foundations (3 of 4)\n",
    "\n",
    "Additionally, since we use independent random samples that have finite non-zero variances, we know that the resulting distribution of our estimates will be approximately *normal*\n",
    "\n",
    "$$ X_i = X_1, X_2, \\dotsc \\qquad \\mathbb{E}[X_i] = \\mu \\qquad \\operatorname{Var}(X_i) = \\sigma^{2} \\in (0,\\infty)$$\n",
    "\n",
    "This will be the case regardless of what the underlaying distribution of $X$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mathematical Foundations (4 of 4)\n",
    "\n",
    "The justification that the resulting estimates will follow a normal distribution is the Central Limit Theorem (Laplace, 1810)\n",
    "\n",
    "Let $X_1, X_2, \\dotsc$ be independent, identically distributed random variables with\n",
    "\n",
    "$$\\mathbb{E}[X_i] = \\mu \\qquad \\operatorname{Var}(X_i) = \\sigma^{2} \\in (0,\\infty)$$\n",
    "\n",
    "If we define the standardized sum $Z_n$ then as $n \\to \\infty$\n",
    "\n",
    "$$Z_n = \\frac{\\,\\sum_{i=1}^{n} X_i - n\\mu}{\\sigma\\sqrt{n}} \\;\\xrightarrow{\\;\\mathcal{D}\\;}\\; \\mathcal{N}(0,1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Monte Carlo Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Variants (1 of 6)\n",
    "### \"Crude\" Monte Carlo Simulation\n",
    "\n",
    "The most common type of Monte Carlo simulation is typically referred to as \"crude\" (aka \"simple\" or \"naive\") Monte Carlo simulation\n",
    "\n",
    "The goal if this variant is to approximate an expected value or an integral of the form\n",
    "\n",
    "$$\\mu \\;=\\; \\mathbb{E}_{X\\sim p}[f(X)] \\;=\\; \\int_{\\Omega} f(x)\\,p(x)\\,dx$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Variants (2 of 6)\n",
    "\n",
    "This method is used when $\\mu$ is difficult or impossible to compute analytically\n",
    "\n",
    "It generates\n",
    "\n",
    "- Estimate: $\\hat{\\mu}_n$\n",
    "\n",
    "- Standard Error: $\\displaystyle \\frac{\\hat{\\sigma}}{\\sqrt{n}}$, where $\\hat{\\sigma}^2$ is the sample variance of $f(X_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Variants (3 of 6)\n",
    "### Markov Chain Monte Carlo Simulation (MCMC)\n",
    "\n",
    "Markov‑Chain Monte Carlo is a family of algorithms for sampling randomly from a complicated probability distribution when you cannot draw independent samples directly\n",
    "\n",
    "It combines both\n",
    "\n",
    "1. Markov chains – a random walk whose next step depends only on the current state, not on the whole past\n",
    "\n",
    "2. Monte Carlo – estimating integrals or expectations using random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Variants (4 of 6)\n",
    "Two of the most famous and widely used MCMC algorithms are\n",
    "\n",
    "1. The Metropolis–Hastings algorithm\n",
    "\n",
    "$$P(x,y)=q(x,y) \\alpha(x,y) \\qquad \\alpha(x,y)=\\min{1,\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}}$$\n",
    "\n",
    "\n",
    "2. Gibbs sampling\n",
    "\n",
    "$$X_{k}^{(t+1)} \\sim \\pi \\left( x_{k} | x_{−k}^{(t)} \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Variants (5 of 6)\n",
    "### Sequential Monte Carlo Simulation (SMC)\n",
    "\n",
    "Sequential Monte Carlo (SMC), also called \"particle filtering\", is sort of like Monte Carlo *on the fly*\n",
    "\n",
    "Instead of drawing one big batch of random samples and keeping them forever, SMC keeps a \"cloud of particles\" that move and change weights as new information arrives\n",
    "\n",
    "At every time‑step the cloud is adjusted so that, as a group, the particles behave like random draws from the \"current\" probability distribution we care about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Variants (6 of 6)\n",
    "\n",
    "A typical SMC model\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_0 &\\sim p(x_0) &\\text{(initial state)}\\\\\n",
    "x_t &\\sim f(x_t\\mid x_{t-1}) &\\text{(state evolution)}\\\\\n",
    "y_t &\\sim g(y_t\\mid x_t) &\\text{(noisy observation)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For $i=1,\\dots,N$ particles\n",
    "\n",
    "$$\n",
    "w_t^{(i)} \\;=\\;\n",
    "w_{t-1}^{(i)}\\;\n",
    "\\frac{\\,g\\bigl(y_t\\mid x_t^{(i)}\\bigr)\\,f\\bigl(x_t^{(i)}\\mid x_{t-1}^{(i)}\\bigr)}\n",
    "     {q_t\\!\\bigl(x_t^{(i)}\\mid x_{0:t-1}^{(i)},y_{0:t}\\bigr)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variance Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance Reduction Methods (1 of 5)\n",
    "\n",
    "The purpose of variance reductions methods is to essentially reduce $\\sigma^2$ without increasing $n$\n",
    "\n",
    "In essence, this provides you with either the same level of accuracy in your result by using far fewer samples or higher accuracy for the same cost\n",
    "\n",
    "Also, note that each technique is unbiased and independent of the others, as such you can often combine two or more different methods for extra \"power\"\n",
    "\n",
    "Overall, they yield faster convergence, tighter confidence intervals, and less computing time for the same precision\n",
    "\n",
    "There are four classic variance reduction techniques..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance Reduction Methods (2 of 5)\n",
    "\n",
    "### 1. Antithetic variates\n",
    "\n",
    "Works by using pairs of negatively correlated samples so their errors cancel\n",
    "\n",
    "1. Generate a random number: $U\\sim\\text{Uniform}(0,1)$\n",
    "\n",
    "2. Create its “mirror”: $\\tilde U = 1-U$\n",
    "\n",
    "3. Evaluate both: $f(U),\\;f(\\tilde U)$\n",
    "\n",
    "4. Average them:\n",
    "\n",
    "   $$\n",
    "   \\hat{\\mu}_{\\text{anti}}=\\frac{f(U)+f(\\tilde U)}{2}\n",
    "   $$\n",
    "\n",
    "Because $U$ and $1-U$ are *negatively correlated*, upward errors in one tend to be balanced by downward errors in the other, often halving the variance or better for monotone $f$\n",
    "\n",
    "An analogy would be if you guess too high on one coin‑flip and then guess low on its opposite and they sort of cancel each other out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance Reduction Methods (3 of 5)\n",
    "\n",
    "### 2. Control variates\n",
    "\n",
    "Works by borrowing information from a *related quantity* whose mean you already know\n",
    "\n",
    "Suppose you can compute $\\mathbb{E}[Y]=\\nu$ exactly and $Y$ is correlated with $f(X)$\n",
    "\n",
    "1. Simulate pairs $(f(X_i),Y_i)$\n",
    "2. Form the *control‑variates estimator*\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{\\text{cv}}\n",
    "\\;=\\;\n",
    "\\bar f\n",
    "\\;-\\;\n",
    "c\\,(\\bar Y-\\nu)\n",
    "\\qquad\n",
    "\\text{with } c^\\star=\\frac{\\operatorname{Cov}(f,Y)}{\\operatorname{Var}(Y)}\n",
    "$$\n",
    "\n",
    "Choosing $c=c^\\star$ gives the minimum variance\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}_{\\text{cv}})=\n",
    "\\bigl(1-\\rho^{2}\\bigr)\\frac{\\sigma^{2}}{n}\n",
    "$$\n",
    "\n",
    "where $\\rho$ is the correlation between $f$ and $Y$\n",
    "\n",
    "An analogy would be if you know today’s temperature exactly, you can use that fact to correct a noisy thermometer reading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance Reduction Methods (4 of 5)\n",
    "\n",
    "### 3. Importance sampling\n",
    "\n",
    "Works by sampling more often where $f$ is large and re‑weighting to stay unbiased\n",
    "\n",
    "The goal is to focus samples where they “matter”\n",
    "\n",
    "1. Pick an *easy‑to‑sample proposal* density $q(x)$ that puts more mass in important regions\n",
    "\n",
    "2. Draw $X_i\\sim q$\n",
    "\n",
    "3. Weight each sample by\n",
    "\n",
    "   $$\n",
    "   w_i=\\frac{p(X_i)}{q(X_i)} \\quad (\\text{likelihood ratio})\n",
    "   $$\n",
    "\n",
    "4. Estimator:\n",
    "\n",
    "   $$\n",
    "   \\hat{\\mu}_{\\text{IS}}\n",
    "   \\;=\\;\n",
    "   \\frac{1}{n}\\sum_{i=1}^{n} f(X_i)w_i\n",
    "   $$\n",
    "\n",
    "Unbiased because the weight corrects for sampling from the “wrong” distribution\n",
    "\n",
    "An analogy would be if rare events drive the result then deliberately looking for them and giving each discovery its correct statistical weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance Reduction Methods (5 of 5)\n",
    "\n",
    "### 4. Stratified or Latin hypercube sampling\n",
    "\n",
    "Works by forcing samples to *cover every region* of the input space evenly\n",
    "\n",
    "Stratification is accomplished by spliting the input range into $k$ *strata* (equal‑length intervals in 1‑D, equal‑volume boxes in multi‑D)\n",
    "\n",
    "Then sampling $n_j$ points inside stratum $j$ and use a weighted average:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{\\text{strat}}\n",
    "=\\sum_{j=1}^{k} p_j\\,\\bar f_j\n",
    "\\qquad\n",
    "p_j=\\Pr\\{X\\text{ falls in stratum }j\\}\n",
    "$$\n",
    "\n",
    "This ensures every slice is represented\n",
    "\n",
    "This also yields lower variance, especially when $f$ differs a lot across slices\n",
    "\n",
    "An analogy would be if you had a variety box of chocolates with the various types of candy grouped together into their own sections and you then took one chocolate from each section of the box instead of blind‑grabbing and maybe missing whole flavours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will be focusing on \"simple\" Monte Carlo because it is the easiest variant to both understand and code\n",
    "\n",
    "Remember that \"simple\" Monte Carlo works by\n",
    "\n",
    "1. Drawing *i.i.d.* random samples $X_1,\\dots,X_n$ from a distribution $X$\n",
    "\n",
    "2. Computing $f(X_i)$ for each $i$\n",
    "\n",
    "3. Returning a estimator $ \\mathbb{E}[X_i] \\approx \\hat\\mu_n = \\frac1n\\sum f(X_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1: Estimating $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this example, we will be estimating the value of $\\pi$ by \"throwing darts\"\n",
    "\n",
    "We assume that our target is a circle enclosed by a square of the same diameter\n",
    "\n",
    "In our case, that means a hit is a value that falls within the circle, and a miss is a value that falls outside the circle\n",
    "\n",
    "We then select random samples, bounded by the areas of the outer square, and determine which ones are hits versus misses\n",
    "\n",
    "To make things a bit easier, we split the circle/square into quarters\n",
    "\n",
    "We can then use our hits to estimate the value of $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Geometric Intuition\n",
    "\n",
    "1. A *unit circle* has area:\n",
    "\n",
    "$$\n",
    "A_{\\text{circle}} = \\pi r^2 = \\pi \\cdot 1^2 = \\pi\n",
    "$$\n",
    "\n",
    "2. A quarter of that circle (in the first quadrant) has area:\n",
    "\n",
    "$$\n",
    "A_{\\text{quarter}} = \\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "3. The square that bounds it from $(0, 0)$ to $(1, 1)$ has area:\n",
    "\n",
    "$$\n",
    "A_{\\text{square}} = 1\n",
    "$$\n",
    "\n",
    "4. The probability that a random point (x, y) in $[0,1]^2$ lies inside the quarter-circle is:\n",
    "\n",
    "$$\n",
    "P(x^2 + y^2 < 1) = \\frac{\\pi}{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of points to sample\n",
    "n_samples = 1_000  # (You can increase this if desired)\n",
    "\n",
    "# Generate random points in [0, 1] x [0, 1]\n",
    "x = np.random.rand(n_samples)\n",
    "y = np.random.rand(n_samples)\n",
    "\n",
    "# Determine which points are inside the quarter-circle\n",
    "inside = x**2 + y**2 < 1\n",
    "\n",
    "# Estimate π\n",
    "pi_estimate = 4 * np.mean(inside)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x[inside], y[inside], color='blue', s=1, label='Inside Quarter-Circle')\n",
    "plt.scatter(x[~inside], y[~inside], color='red', s=1, label='Outside Quarter-Circle')\n",
    "plt.plot(np.linspace(0, 1, 1000), np.sqrt(1 - np.linspace(0, 1, 1000)**2), color='black', lw=1, label='Quarter Circle')\n",
    "plt.title(f\"Monte Carlo Estimation of π\\nEstimated π ≈ {pi_estimate:.6f}\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.axis(\"square\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mc_pi(n_samples=100_000):\n",
    "    x = np.random.rand(n_samples)\n",
    "    y = np.random.rand(n_samples)\n",
    "    return 4 * np.mean(x**2 + y**2 < 1)\n",
    "\n",
    "for n in [10**k for k in range(1, 8)]:\n",
    "    print(f\"{n:>10} samples: pi ≈ {mc_pi(n):.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "# Plain Monte Carlo\n",
    "@njit\n",
    "def mc_pi_plain(n_samples):\n",
    "    x = np.random.rand(n_samples)\n",
    "    y = np.random.rand(n_samples)\n",
    "    return 4 * np.mean(x**2 + y**2 < 1)\n",
    "\n",
    "# Antithetic Variates\n",
    "@njit\n",
    "def mc_pi_antithetic(n_samples):\n",
    "    x = np.random.rand(n_samples)\n",
    "    y = np.random.rand(n_samples)\n",
    "    x_anti = 1 - x\n",
    "    y_anti = 1 - y\n",
    "    count = 0\n",
    "    for i in range(n_samples):\n",
    "        if x[i]**2 + y[i]**2 < 1:\n",
    "            count += 1\n",
    "        if x_anti[i]**2 + y_anti[i]**2 < 1:\n",
    "            count += 1\n",
    "    return 4 * count / (2 * n_samples)\n",
    "\n",
    "# Control Variates (using g(x, y) = x + y, E[g] = 1)\n",
    "@njit\n",
    "def cov(x, y):\n",
    "    mx = np.mean(x)\n",
    "    my = np.mean(y)\n",
    "    return np.sum((x - mx) * (y - my)) / (len(x) - 1)\n",
    "\n",
    "@njit\n",
    "def mc_pi_control_variate(n_samples):\n",
    "    x = np.random.rand(n_samples)\n",
    "    y = np.random.rand(n_samples)\n",
    "    f = (x**2 + y**2 < 1).astype(np.float64)\n",
    "    g = x + y\n",
    "    beta = cov(f, g) / np.var(g)\n",
    "    return 4 * (np.mean(f) - beta * (np.mean(g) - 1.0))\n",
    "\n",
    "# Importance Sampling using polar coordinates\n",
    "@njit\n",
    "def mc_pi_importance_sampling(n_samples):\n",
    "    r = np.sqrt(np.random.rand(n_samples))  # sample r^2 uniformly\n",
    "    theta = np.random.rand(n_samples) * (np.pi / 2)\n",
    "    weights = 1 / (2 * r)\n",
    "    return 4 * np.mean(weights)\n",
    "\n",
    "# Stratified Sampling\n",
    "@njit\n",
    "def mc_pi_stratified(k):\n",
    "    step = 1.0 / k\n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            x = (i + np.random.rand()) * step\n",
    "            y = (j + np.random.rand()) * step\n",
    "            if x*x + y*y < 1:\n",
    "                count += 1\n",
    "    return 4 * count / (k * k)\n",
    "\n",
    "# Run comparison\n",
    "print(f\"{'Samples':>12} | {'Simple MC':>10} | {'Antithetic Var':>10} | {'Control Var':>11} | {'Importance Samp':>11} | {'Stratified Samp':>11}\")\n",
    "print(\"-\" * 100)\n",
    "for n in [10**k for k in range(1, 7)]:\n",
    "    k = int(np.sqrt(n))\n",
    "    pi_plain = mc_pi_plain(n)\n",
    "    pi_anti = mc_pi_antithetic(n)\n",
    "    pi_cv = mc_pi_control_variate(n)\n",
    "    pi_is = mc_pi_importance_sampling(n)\n",
    "    pi_strat = mc_pi_stratified(k)\n",
    "    print(f\"{n:>12} | {pi_plain:10.8f} | {pi_anti:14.8f} | {pi_cv:11.8f} | {pi_is:15.8f} | {pi_strat:10.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can see here how\n",
    "\n",
    "- Stratified sampling tends to give the most stable and accurate estimates at low sample sizes\n",
    "\n",
    "- Control variates smooth the estimates if the correlation is strong\n",
    "\n",
    "- Antithetics help a bit, especially at smaller scales\n",
    "\n",
    "- Importance sampling may perform inconsistently due to the weighting method and geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Roulette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a \"real\" American roulette game:\n",
    "\n",
    "* The wheel has 38 slots: numbers 1–36, 0, and 00\n",
    "\n",
    "* Bets can be placed on:\n",
    "\n",
    "  * Single numbers\n",
    "  * Red or black\n",
    "  * Odd or even\n",
    "  * Dozens, columns, etc.\n",
    "  \n",
    "* For red/black betting:\n",
    "\n",
    "  * Half the numbers (18) are red\n",
    "  * 18 are black\n",
    "  * 0 and 00 are **neither**, and cause losses for even-money bets\n",
    "\n",
    "Note that the *house edge* comes from those 0 and 00 slots, reducing the winning probability for red from 50% down to **18/38 ≈ 47.37%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our simulation we will only be red/black betting\n",
    "\n",
    "We \"play\" the game by\n",
    "\n",
    "* Spinning the wheel (`spin_roulette()`)\n",
    "\n",
    "* Checking whether or not the result is red (`is_red()`)\n",
    "\n",
    "* Adjusting the player’s balance based on:\n",
    "\n",
    "  * A win (red): gain bet amount\n",
    "  \n",
    "  * A loss (black, 0, or 00): lose bet amount\n",
    "  \n",
    "* Tracking the bankroll for a number of spins or until bankruptcy\n",
    "\n",
    "This will allow us to estimate\n",
    "\n",
    "* Expected profits/losses for each betting system\n",
    "\n",
    "* Risk of ruin (bankruptcy) for each betting system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Also, in our simulation we will be comparing four different betting strategies\n",
    "\n",
    "####  1. Flat Betting\n",
    "\n",
    "* You bet a fixed amount (e.g., \\$1) every spin, regardless of the outcome\n",
    "\n",
    "* Safe and steady, with predictable loss over time due to the house edge\n",
    "\n",
    "#### 2. Martingale Betting\n",
    "\n",
    "* After a loss you double your next bet\n",
    "\n",
    "* After a win you *reset* to the base bet\n",
    "\n",
    "* The goal is to recover all previous losses + 1 unit.\n",
    "\n",
    "* Danger: after several losses in a row, your bet becomes huge and may **exceed your balance** (bankruptcy).\n",
    "\n",
    "#### 3. Reverse Martingale\n",
    "\n",
    "* After a win you double your next bet\n",
    "\n",
    "* After a loss you *reset* to the base bet\n",
    "\n",
    "* The goal is to capitalize on winning streaks while minimizing losses\n",
    "\n",
    "* It is \"safer\" than Martingale betting, but depends on hitting streaks to profit\n",
    "\n",
    "#### 4. Random Betting\n",
    "\n",
    "* At each spin, you randomly pick a bet size between \\$1 and \\$10 (as long as your balance allows)\n",
    "\n",
    "* Simulates unstructured, impulsive play\n",
    "\n",
    "* Performance varies wildly — some trials do well, others go broke quickly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Red numbers for American roulette (NumPy array for speed)\n",
    "RED_NUMBERS = np.array([\n",
    "    1, 3, 5, 7, 9, 12, 14, 16, 18,\n",
    "    19, 21, 23, 25, 27, 30, 32, 34, 36\n",
    "])\n",
    "\n",
    "# Variables\n",
    "purse = 100\n",
    "start = 1\n",
    "spins = 100\n",
    "\n",
    "\n",
    "def spin_roulette():\n",
    "    return np.random.randint(0, 38)  # 0–36 and 37 represents '00'\n",
    "\n",
    "def is_red(number):\n",
    "    return number in RED_NUMBERS\n",
    "\n",
    "def simulate_flat(initial_balance=purse, bet=start, max_spins=spins):\n",
    "    balance = initial_balance\n",
    "    path = [balance]\n",
    "    for _ in range(max_spins):\n",
    "        if balance < bet:\n",
    "            break\n",
    "        result = spin_roulette()\n",
    "        balance += bet if is_red(result) else -bet\n",
    "        path.append(balance)\n",
    "    return path\n",
    "\n",
    "def simulate_martingale(initial_balance=purse, base_bet=start, max_spins=spins):\n",
    "    balance = initial_balance\n",
    "    path = [balance]\n",
    "    bet = base_bet\n",
    "    for _ in range(max_spins):\n",
    "        if balance < bet:\n",
    "            break\n",
    "        result = spin_roulette()\n",
    "        if is_red(result):\n",
    "            balance += bet\n",
    "            bet = base_bet\n",
    "        else:\n",
    "            balance -= bet\n",
    "            bet *= 2\n",
    "        path.append(balance)\n",
    "    return path\n",
    "\n",
    "def simulate_reverse_martingale(initial_balance=purse, base_bet=start, max_spins=spins):\n",
    "    balance = initial_balance\n",
    "    path = [balance]\n",
    "    bet = base_bet\n",
    "    for _ in range(max_spins):\n",
    "        if balance < bet:\n",
    "            break\n",
    "        result = spin_roulette()\n",
    "        if is_red(result):\n",
    "            balance += bet\n",
    "            bet *= 2\n",
    "        else:\n",
    "            balance -= bet\n",
    "            bet = base_bet\n",
    "        path.append(balance)\n",
    "    return path\n",
    "\n",
    "def simulate_random(initial_balance=purse, max_spins=spins):\n",
    "    balance = initial_balance\n",
    "    path = [balance]\n",
    "    for _ in range(max_spins):\n",
    "        if balance < 1:\n",
    "            break\n",
    "        bet = np.random.randint(1, min(11, balance + 1))  # $1–$10 or all remaining\n",
    "        result = spin_roulette()\n",
    "        balance += bet if is_red(result) else -bet\n",
    "        path.append(balance)\n",
    "    return path\n",
    "\n",
    "# Simulate one trial for each strategy\n",
    "flat_path = simulate_flat()\n",
    "martingale_path = simulate_martingale()\n",
    "reverse_path = simulate_reverse_martingale()\n",
    "random_path = simulate_random()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(flat_path, label='Flat Betting')\n",
    "plt.plot(martingale_path, label='Martingale')\n",
    "plt.plot(reverse_path, label='Reverse Martingale')\n",
    "plt.plot(random_path, label='Random Betting')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8, label='Bankruptcy')\n",
    "plt.title('Roulette Betting Strategies – Single Trial Bankroll Over Time')\n",
    "plt.xlabel('Spin Number')\n",
    "plt.ylabel('Balance ($)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "# Red numbers array\n",
    "RED_NUMBERS = np.array([\n",
    "    1, 3, 5, 7, 9, 12, 14, 16, 18,\n",
    "    19, 21, 23, 25, 27, 30, 32, 34, 36\n",
    "])\n",
    "\n",
    "@njit\n",
    "def spin_roulette():\n",
    "    return np.random.randint(0, 38)  # 0–36, 37 = '00'\n",
    "\n",
    "@njit\n",
    "def is_red(n):\n",
    "    if n == 0 or n == 37:\n",
    "        return False\n",
    "    for r in RED_NUMBERS:\n",
    "        if r == n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "@njit\n",
    "def flat_bet(balance, bet, spins):\n",
    "    for _ in range(spins):\n",
    "        if balance < bet:\n",
    "            break\n",
    "        result = spin_roulette()\n",
    "        if is_red(result):\n",
    "            balance += bet\n",
    "        else:\n",
    "            balance -= bet\n",
    "    return balance\n",
    "\n",
    "@njit\n",
    "def martingale(balance, base_bet, spins):\n",
    "    bet = base_bet\n",
    "    for _ in range(spins):\n",
    "        if balance < bet:\n",
    "            break\n",
    "        result = spin_roulette()\n",
    "        if is_red(result):\n",
    "            balance += bet\n",
    "            bet = base_bet\n",
    "        else:\n",
    "            balance -= bet\n",
    "            bet *= 2\n",
    "    return balance\n",
    "\n",
    "@njit\n",
    "def reverse_martingale(balance, base_bet, spins):\n",
    "    bet = base_bet\n",
    "    for _ in range(spins):\n",
    "        if balance < bet:\n",
    "            break\n",
    "        result = spin_roulette()\n",
    "        if is_red(result):\n",
    "            balance += bet\n",
    "            bet *= 2  # press win\n",
    "        else:\n",
    "            balance -= bet\n",
    "            bet = base_bet\n",
    "    return balance\n",
    "\n",
    "@njit\n",
    "def random_betting(balance, base_bet, spins):\n",
    "    for _ in range(spins):\n",
    "        bet = np.random.randint(1, max(2, min(10, int(balance))))\n",
    "        if balance < bet:\n",
    "            break\n",
    "        result = spin_roulette()\n",
    "        if is_red(result):\n",
    "            balance += bet\n",
    "        else:\n",
    "            balance -= bet\n",
    "    return balance\n",
    "\n",
    "# Number of trials\n",
    "num_trials = 10_000\n",
    "\n",
    "# Number of spins\n",
    "num_spins = 10\n",
    "\n",
    "# Starting bet in dollars\n",
    "start_bet = 1\n",
    "\n",
    "# Starting balance in dollars\n",
    "purse = 100\n",
    "\n",
    "@njit\n",
    "def simulate_all_strategies(n_trials=num_trials, spins=num_spins, base_bet=start_bet, initial_balance=purse):\n",
    "    flat = np.zeros(n_trials)\n",
    "    mart = np.zeros(n_trials)\n",
    "    revm = np.zeros(n_trials)\n",
    "    rand = np.zeros(n_trials)\n",
    "    for i in range(n_trials):\n",
    "        flat[i] = flat_bet(initial_balance, base_bet, spins)\n",
    "        mart[i] = martingale(initial_balance, base_bet, spins)\n",
    "        revm[i] = reverse_martingale(initial_balance, base_bet, spins)\n",
    "        rand[i] = random_betting(initial_balance, base_bet, spins)\n",
    "    return flat, mart, revm, rand\n",
    "\n",
    "flat, mart, revm, rand = simulate_all_strategies()\n",
    "\n",
    "print(f\"Starting Balance = ${purse:.2f},  Number of spins = {num_spins},  Number of Trials = {num_trials}\")\n",
    "print()\n",
    "print(f\"Flat Betting    - Avg Balance: ${np.mean(flat):.2f} | Bankruptcy: {np.mean(flat <= 0):.2%}\")\n",
    "print(f\"Martingale      - Avg Balance: ${np.mean(mart):.2f} | Bankruptcy: {np.mean(mart <= 0):.2%}\")\n",
    "print(f\"Reverse Marting - Avg Balance: ${np.mean(revm):.2f} | Bankruptcy: {np.mean(revm <= 0):.2%}\")\n",
    "print(f\"Random Betting  - Avg Balance: ${np.mean(rand):.2f} | Bankruptcy: {np.mean(rand <= 0):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We have\n",
    "\n",
    "- Provided some history behind where Monte Carlo simulation originated\n",
    "\n",
    "- Explained what Monte Carlo simulation is and *in general* how it works\n",
    "\n",
    "- Identified some of the mathematics - specifically probability - underlaying it\n",
    "\n",
    "- Shown some different varieties of Monte Carlo methods\n",
    "\n",
    "- Shown some methods for improving the performance of Monte Carlo simulations\n",
    "\n",
    "- Demonstrated via working examples how simulations can be used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some references for further study\n",
    "- Metropolis, N. & Ulam, S. (1949). *The Monte Carlo Method*. JASA.\n",
    "- Kalos, M. H. & Whitlock, P. A. (2008). *Monte Carlo Methods*. Wiley.\n",
    "- Robert, C. P. & Casella, G. (2004). *Monte Carlo Statistical Methods*. Springer.\n",
    "- Glasserman, P. (2003). *Monte Carlo Methods in Financial Engineering*. Springer.\n",
    "- Owen, A. B. (2013). *Monte Carlo Theory, Methods and Examples*."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "livereveal": {
   "autolaunch": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
